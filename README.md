<div align="center">

# ğŸ§¬ Mental Health Prediction using Genetic Algorithm Optimized Ensemble Learning

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-1.0+-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)](https://scikit-learn.org)
[![XGBoost](https://img.shields.io/badge/XGBoost-1.7+-EC4E20?style=for-the-badge&logo=xgboost&logoColor=white)](https://xgboost.ai)
[![LightGBM](https://img.shields.io/badge/LightGBM-4.0+-9ACD32?style=for-the-badge)](https://lightgbm.readthedocs.io)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

<h3>ğŸ† State-of-the-Art Multi-Class Text Classification for Mental Health Detection</h3>

<p><em>An AI-powered system that classifies mental health conditions from text using a genetically optimized ensemble of 5 machine learning models, achieving <strong>79.34% accuracy</strong> across 7 psychological categories.</em></p>

[ğŸ“Š View Results](#-model-performance-results) â€¢ [ğŸš€ Quick Start](#-quick-start) â€¢ [ğŸ“– Documentation](#-methodology--approach) â€¢ [ğŸ¤ Contributing](#-contributing)

---

<img src="https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png" alt="rainbow line" width="100%">

</div>

## ğŸ“‹ Table of Contents

- [ğŸ¯ Problem Statement](#-problem-statement)
- [ğŸ’¡ Solution Overview](#-solution-overview)
- [ğŸ—ï¸ System Architecture](#ï¸-system-architecture)
- [ğŸ”¬ Methodology & Approach](#-methodology--approach)
  - [Data Pipeline](#1-data-pipeline)
  - [Feature Engineering](#2-feature-engineering-tf-idf)
  - [Model Training](#3-model-training--base-learners)
  - [Genetic Algorithm Optimization](#4-genetic-algorithm-optimization)
  - [Stacking Ensemble](#5-stacking-ensemble-architecture)
- [ğŸ“Š Model Performance Results](#-model-performance-results)
- [ğŸ§¬ The Genetic Algorithm](#-the-genetic-algorithm-explained)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ¨ Visualizations](#-visualizations)
- [ğŸ”® Future Improvements](#-future-improvements)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“œ License](#-license)

---

## ğŸ¯ Problem Statement

<table>
<tr>
<td width="60%">

### The Challenge

Mental health issues are increasingly prevalent, yet early detection remains challenging. People often express their struggles through textâ€”journals, social media posts, chat messagesâ€”using **ambiguous and nuanced language**.

**Key Difficulties:**
- ğŸ”€ **Semantic Overlap**: "Stressed" vs "Anxious" vs "Depressed" share similar vocabulary
- ğŸ­ **Contextual Ambiguity**: "I want to end this" could mean many things
- âš–ï¸ **Class Imbalance**: Rare conditions (Personality Disorder: 2.3%) vs common (Normal: 31%)
- ğŸ“ **Unstructured Data**: Free-form text with typos, slang, and emojis

</td>
<td width="40%">

### The Goal

Build an **AI system** that:

âœ… Classifies text into **7 mental health categories**

âœ… Achieves **high recall** for critical cases (Suicidal)

âœ… Handles **severe class imbalance**

âœ… Runs efficiently on **CPU hardware**

âœ… Provides **interpretable results**

</td>
</tr>
</table>

### ğŸ·ï¸ Classification Categories

| Category | Description | Dataset % |
|:--------:|:------------|:---------:|
| ğŸŸ¢ **Normal** | Healthy emotional state | 31.0% |
| ğŸ”µ **Depression** | Persistent sadness, hopelessness | 29.2% |
| ğŸ”´ **Suicidal** | Self-harm ideation or intent | 20.2% |
| ğŸŸ¡ **Anxiety** | Excessive worry, panic symptoms | 7.4% |
| ğŸŸ£ **Bipolar** | Mood swings, manic episodes | 5.5% |
| ğŸŸ  **Stress** | Overwhelm, burnout indicators | 5.1% |
| âšª **Personality Disorder** | Complex behavioral patterns | 2.3% |

---

## ğŸ’¡ Solution Overview

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ§¬ GENETIC ALGORITHM OPTIMIZED ENSEMBLE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚Logistic â”‚   â”‚  Linear â”‚   â”‚ LightGBMâ”‚   â”‚ XGBoost â”‚   â”‚  Naive  â”‚     â”‚
â”‚   â”‚Regressionâ”‚   â”‚   SVM   â”‚   â”‚ Booster â”‚   â”‚  Trees  â”‚   â”‚  Bayes  â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â”‚
â”‚        â”‚             â”‚             â”‚             â”‚             â”‚           â”‚
â”‚        â”‚    P(y|x)   â”‚    P(y|x)   â”‚    P(y|x)   â”‚    P(y|x)   â”‚   P(y|x) â”‚
â”‚        â–¼             â–¼             â–¼             â–¼             â–¼           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚           ğŸ§¬ GENETIC ALGORITHM WEIGHT OPTIMIZATION               â”‚     â”‚
â”‚   â”‚                                                                   â”‚     â”‚
â”‚   â”‚   Population: 50 â”‚ Generations: 30 â”‚ Fitness: F1-Weighted        â”‚     â”‚
â”‚   â”‚                                                                   â”‚     â”‚
â”‚   â”‚   Final Weights: [0.246, 0.257, 0.203, 0.133, 0.161]            â”‚     â”‚
â”‚   â”‚                  (LGBM, XGB, SVM, LogReg, NB)                     â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                               â”‚                                           â”‚
â”‚                               â–¼                                           â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚                    â”‚  FINAL PREDICTION   â”‚                               â”‚
â”‚                    â”‚   Accuracy: 79.34%  â”‚                               â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### ğŸŒŸ Key Innovations

| Innovation | Description | Impact |
|:-----------|:------------|:-------|
| **ğŸ§¬ Genetic Algorithm Fusion** | Evolutionary optimization of model weights | +2.5% accuracy over simple averaging |
| **ğŸ“Š TF-IDF with Bigrams** | Captures "not happy" â‰  "happy" | Essential for negation handling |
| **âš–ï¸ Balanced Class Weights** | 6.9x multiplier for rare classes | +15% minority class recall |
| **ğŸ—ï¸ Stacking Architecture** | Meta-learner over base predictions | Leverages model diversity |
| **ğŸ”¬ Hierarchical Classification** | Binary â†’ Multi-class cascade | Improved "Normal" detection |

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              SYSTEM ARCHITECTURE                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                â”‚
â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                  â”‚
â”‚  â•‘   RAW TEXT    â•‘ -> â•‘   CLEANING    â•‘ -> â•‘   TF-IDF      â•‘                  â”‚
â”‚  â•‘   INPUT       â•‘    â•‘   PIPELINE    â•‘    â•‘   VECTORIZER  â•‘                  â”‚
â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                  â”‚
â”‚         â”‚                    â”‚                    â”‚                            â”‚
â”‚         â”‚   â€¢ Lowercase      â”‚   â€¢ 6,000 features â”‚                            â”‚
â”‚         â”‚   â€¢ Remove URLs    â”‚   â€¢ Unigrams +     â”‚                            â”‚
â”‚         â”‚   â€¢ Remove noise   â”‚     Bigrams        â”‚                            â”‚
â”‚         â”‚   â€¢ Normalize      â”‚   â€¢ min_df=5       â”‚                            â”‚
â”‚         â–¼                    â–¼                    â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                    ENSEMBLE OF 5 BASE MODELS                          â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚                                                                       â”‚     â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚     â”‚
â”‚  â”‚   â”‚  LOGISTIC   â”‚  â”‚   LINEAR    â”‚  â”‚  LIGHTGBM   â”‚                  â”‚     â”‚
â”‚  â”‚   â”‚ REGRESSION  â”‚  â”‚    SVM      â”‚  â”‚   BOOSTER   â”‚                  â”‚     â”‚
â”‚  â”‚   â”‚   (SAGA)    â”‚  â”‚ (Calibrated)â”‚  â”‚  (100 trees)â”‚                  â”‚     â”‚
â”‚  â”‚   â”‚  w=0.133    â”‚  â”‚   w=0.203   â”‚  â”‚   w=0.246   â”‚                  â”‚     â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚     â”‚
â”‚  â”‚                                                                       â”‚     â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚     â”‚
â”‚  â”‚   â”‚   XGBOOST   â”‚  â”‚   NAIVE     â”‚                                   â”‚     â”‚
â”‚  â”‚   â”‚   (350      â”‚  â”‚   BAYES     â”‚                                   â”‚     â”‚
â”‚  â”‚   â”‚   trees)    â”‚  â”‚ (Complement)â”‚                                   â”‚     â”‚
â”‚  â”‚   â”‚   w=0.257   â”‚  â”‚   w=0.161   â”‚                                   â”‚     â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚     â”‚
â”‚  â”‚                                                                       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                    â”‚                                          â”‚
â”‚                                    â–¼                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                  ğŸ§¬ GENETIC ALGORITHM OPTIMIZER                       â”‚     â”‚
â”‚  â”‚                                                                       â”‚     â”‚
â”‚  â”‚   â€¢ Population: 50 weight vectors                                     â”‚     â”‚
â”‚  â”‚   â€¢ Generations: 30 evolution cycles                                  â”‚     â”‚
â”‚  â”‚   â€¢ Selection: Top-10 survive                                        â”‚     â”‚
â”‚  â”‚   â€¢ Crossover: Average of parents                                    â”‚     â”‚
â”‚  â”‚   â€¢ Mutation: Â±0.1 random perturbation                               â”‚     â”‚
â”‚  â”‚   â€¢ Fitness: Weighted F1-Score                                       â”‚     â”‚
â”‚  â”‚                                                                       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                    â”‚                                          â”‚
â”‚                                    â–¼                                          â”‚
â”‚                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—                         â”‚
â”‚                    â•‘    FINAL WEIGHTED PREDICTION   â•‘                         â”‚
â”‚                    â•‘                                â•‘                         â”‚
â”‚                    â•‘    P(y) = Î£ wáµ¢ Ã— P(y|Modeláµ¢)  â•‘                         â”‚
â”‚                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                         â”‚
â”‚                                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Methodology & Approach

### 1. Data Pipeline

```
RAW DATA (53,000+ posts)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA CLEANING   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Lowercase       â”‚
â”‚ â€¢ Remove URLs     â”‚
â”‚ â€¢ Remove [deleted]â”‚
â”‚ â€¢ Normalize spacesâ”‚
â”‚ â€¢ Filter < 10 charâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DEDUPLICATION    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Remove exact    â”‚
â”‚   duplicates      â”‚
â”‚ â€¢ Resolve label   â”‚
â”‚   conflicts       â”‚
â”‚ â€¢ 41,000 rows     â”‚
â”‚   remaining       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STRATIFIED SPLIT â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ 80% Training    â”‚
â”‚ â€¢ 20% Testing     â”‚
â”‚ â€¢ Preserves class â”‚
â”‚   distribution    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Feature Engineering (TF-IDF)

**Why TF-IDF over Alternatives?**

| Method | Pros | Cons | Decision |
|:-------|:-----|:-----|:---------|
| **Bag of Words** | Simple | Equal weight to all words | âŒ |
| **TF-IDF** | Weights by importance | Optimal for classical ML | âœ… **CHOSEN** |
| **Word2Vec** | Semantic meaning | Requires deep learning | âŒ |
| **BERT** | State-of-the-art | GPU required, slow | âŒ |

**Configuration:**
```python
TfidfVectorizer(
    max_features=6000,      # Top 6000 vocabulary
    ngram_range=(1, 2),     # Unigrams + Bigrams
    min_df=5,               # Minimum document frequency
    stop_words=None         # Manually cleaned
)
```

**Why Bigrams Matter:**
```
Unigram:  "happy" â†’ POSITIVE
Bigram:   "not happy" â†’ NEGATIVE (Captured!)
```

### 3. Model Training â€” Base Learners

#### ğŸ“ˆ Model Comparison Table

| Model | Accuracy | Training Time | Key Strength |
|:------|:--------:|:-------------:|:-------------|
| Logistic Regression | 76.2% | 12 sec | Linear baseline |
| Linear SVM (Tuned) | 77.8% | 45 sec | Margin separation |
| Complement Naive Bayes | 74.1% | 3 sec | Minority class recall |
| LightGBM | 78.1% | 28 sec | Fast gradient boosting |
| XGBoost (High-Power) | 78.4% | ~3 hours | Complex pattern capture |

#### âš–ï¸ Class Weight Handling

```python
# Computed weights for imbalanced classes
class_weights = {
    'Normal': 1.0,
    'Depression': 1.06,
    'Suicidal': 1.53,
    'Anxiety': 4.21,
    'Bipolar': 5.69,
    'Stress': 6.13,
    'Personality Disorder': 6.92  # â† Highest weight!
}
```

### 4. Genetic Algorithm Optimization

<div align="center">

```
                    ğŸ§¬ GENETIC ALGORITHM EVOLUTION
                    
    Generation 1                      Generation 30
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    [0.20, 0.20, 0.20, 0.20, 0.20]   [0.246, 0.257, 0.203, 0.133, 0.161]
              â”‚                                    â”‚
              â”‚      EVOLUTION                     â”‚
              â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º                   â”‚
              â”‚                                    â”‚
         Random Init                          Optimized!
         Acc: 76.2%                          Acc: 79.34%
```

</div>

**Algorithm Steps:**

1. **Initialize Population**: 50 random weight vectors (sum to 1.0)
2. **Evaluate Fitness**: Calculate accuracy for each weight combination
3. **Selection**: Top 10 performers survive
4. **Crossover**: Children = (Parent1 + Parent2) / 2
5. **Mutation**: Random Â±0.1 perturbation with 20% probability
6. **Repeat**: 30 generations until convergence

**Final Optimized Weights:**
```
LightGBM:   0.246  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
XGBoost:    0.257  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
SVM:        0.203  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
LogReg:     0.133  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
Naive Bayes: 0.161 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
```

### 5. Stacking Ensemble Architecture

```
                        LEVEL-1 (Base Models)
                        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        
    Text â†’ TF-IDF â†’ [LogReg] â†’ Pâ‚(7 classes)
                  â†’ [SVM]    â†’ Pâ‚‚(7 classes)
                  â†’ [LGBM]   â†’ Pâ‚ƒ(7 classes)
                  â†’ [XGB]    â†’ Pâ‚„(7 classes)
                  â†’ [NB]     â†’ Pâ‚…(7 classes)
                        â”‚
                        â–¼
              Concatenate: 5 Ã— 7 = 35 features
                        â”‚
                        â–¼
                    LEVEL-2 (Meta-Learner)
                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                        â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   LightGBM Meta   â”‚
              â”‚   or              â”‚
              â”‚   Logistic Meta   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
                  FINAL PREDICTION
```

---

## ğŸ“Š Model Performance Results

### ğŸ† Final Ensemble Metrics

<div align="center">

| Metric | Score | Description |
|:------:|:-----:|:------------|
| **Accuracy** | **79.34%** | Overall correctness |
| **Macro F1** | **74.2%** | Balanced class performance |
| **Weighted F1** | **79.1%** | Sample-weighted F1 |
| **MCC** | **0.746** | Matthews Correlation Coefficient |
| **Cohen's Kappa** | **0.741** | Agreement above chance |

</div>

### ğŸ“ˆ Per-Class Performance

```
                    Precision    Recall    F1-Score   Support
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Anxiety            0.82       0.78       0.80       778
    Bipolar            0.79       0.81       0.80       575
    Depression         0.74       0.74       0.74      3081
    Normal             0.90       0.94       0.92      3270
    Personality        0.72       0.52       0.60       240
    Stress             0.61       0.61       0.61       534
    Suicidal           0.73       0.71       0.72      2131
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    MACRO AVG          0.76       0.73       0.74     10609
    WEIGHTED AVG       0.79       0.79       0.79     10609
```

### ğŸ“‰ Confusion Matrix Insights

```
                 Predicted
              Anx  Bip  Dep  Nor  Per  Str  Sui
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    Anx  â”‚ 607   15   72   25   12   31   16  â”‚
    Bip  â”‚  12  466   42   15    8   14   18  â”‚
Actual  Dep  â”‚  68   45  2280  89   24  122  453  â”‚
    Nor  â”‚  21   17   79 3074    5   32   42  â”‚
    Per  â”‚  16    9   42   17  125   14   17  â”‚
    Str  â”‚  14   22  109   79   16  326   68  â”‚
    Sui  â”‚  22   28  448   81   18   91 1513  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Observations:**
- âœ… **Normal** class: 94% recall (excellent healthy detection)
- âœ… **Bipolar** class: 81% recall (strong mood detection)
- âš ï¸ **Stress â†” Anxiety**: Some confusion (expected semantic overlap)
- âš ï¸ **Depression â†” Suicidal**: 453 cases of Depression predicted as Suicidal (safe failure)

---

## ğŸ§¬ The Genetic Algorithm Explained

### Why Genetic Algorithm?

Traditional hyperparameter tuning methods like Grid Search explore a **discrete** parameter space. However, ensemble weights are **continuous** values between 0 and 1 that must sum to 1â€”creating an infinite search space.

**Genetic Algorithms solve this by:**
1. Exploring diverse solutions in parallel (population)
2. Converging towards optimal regions (selection pressure)
3. Escaping local minima (mutation)

### Visual Evolution Process

```
Generation 0:  Random population of 50 weight vectors
               Average Fitness: 76.2%
               
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â”‚ â† Scattered
               â”‚ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â—‹ â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Generation 15: Population begins converging
               Average Fitness: 78.1%
               
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚       â—‹â—‹â—‹â—‹â—‹                 â”‚
               â”‚      â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹               â”‚ â† Clustering
               â”‚       â—‹â—‹â—‹â—‹                  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Generation 30: Converged to optimal region
               Best Fitness: 79.34%
               
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚         â—                   â”‚
               â”‚        â—â—â—                  â”‚ â† Converged!
               â”‚         â—                   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mathematical Formulation

**Fitness Function:**
$$\text{Fitness}(w) = \text{Accuracy}\left(\arg\max_y \sum_{i=1}^{5} w_i \cdot P(y|x, \text{Model}_i)\right)$$

**Crossover Operation:**
$$w_{\text{child}} = \frac{w_{\text{parent1}} + w_{\text{parent2}}}{2}$$

**Mutation Operation:**
$$w'_j = \text{clip}(w_j + \mathcal{U}(-0.1, 0.1), 0.01, 1.0)$$
$$w_{\text{final}} = \frac{w'}{\sum w'}$$

---

## ğŸ› ï¸ Tech Stack

<div align="center">

| Category | Technologies |
|:---------|:-------------|
| **Language** | Python 3.8+ |
| **ML Framework** | Scikit-Learn, XGBoost, LightGBM |
| **NLP** | NLTK, TextBlob, TF-IDF |
| **Visualization** | Matplotlib, Seaborn, Plotly |
| **Data Processing** | Pandas, NumPy |
| **Frontend** | Streamlit |
| **Backend** | FastAPI |
| **Notebook** | Jupyter / Google Colab |

</div>

### ğŸ“¦ Dependencies

```
pandas>=1.3.0
numpy>=1.21.0
scikit-learn>=1.0.0
xgboost>=1.7.0
lightgbm>=4.0.0
nltk>=3.6.0
textblob>=0.17.1
matplotlib>=3.5.0
seaborn>=0.12.0
plotly>=5.0.0
joblib>=1.1.0
streamlit>=1.20.0
fastapi>=0.95.0
```

---

## ğŸš€ Quick Start

### 1. Clone the Repository
```bash
git clone https://github.com/Likhith623/Mental-Health-Prediction.git
cd Mental-Health-Prediction
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the Notebook
Open `MENTAL_HEALTH_final.ipynb` in Jupyter or Google Colab and execute cells sequentially.

### 4. Quick Prediction (After Training)
```python
import joblib
import numpy as np

# Load the final model
model = joblib.load('FINAL_MODEL.pkl')
tfidf = model['tfidf']
encoder = model['label_encoder']
weights = model['weights']
models = {k: model[k] for k in ['logreg', 'svm', 'lgbm', 'xgb', 'nb']}

def predict(text):
    vec = tfidf.transform([text.lower()])
    probs = [m.predict_proba(vec) for m in models.values()]
    final_prob = sum(w * p for w, p in zip(weights, probs))
    return encoder.classes_[np.argmax(final_prob)]

# Example
print(predict("I feel so anxious and my heart is racing"))
# Output: Anxiety
```

### 5. Run Streamlit App
```bash
streamlit run app.py
```

---

## ğŸ“ Project Structure

```
Mental-Health-Prediction/
â”‚
â”œâ”€â”€ ğŸ““ MENTAL_HEALTH_final.ipynb    # Main ML pipeline notebook
â”œâ”€â”€ ğŸ““ TEXT_LARGE.ipynb             # Extended text analysis
â”‚
â”œâ”€â”€ ğŸ“ backend/
â”‚   â”œâ”€â”€ main.py                      # FastAPI backend
â”‚   â”œâ”€â”€ requirements.txt             # Backend dependencies
â”‚   â””â”€â”€ README.md                    # Backend documentation
â”‚
â”œâ”€â”€ ğŸ“ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js                   # React main component
â”‚   â”‚   â”œâ”€â”€ App.css                  # Styles
â”‚   â”‚   â””â”€â”€ index.js                 # Entry point
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ index.html               # HTML template
â”‚   â”œâ”€â”€ package.json                 # Frontend dependencies
â”‚   â””â”€â”€ README.md                    # Frontend documentation
â”‚
â”œâ”€â”€ ğŸ“ docs/
â”‚   â”œâ”€â”€ training_models.md           # Model training guide
â”‚   â”œâ”€â”€ adaboost/                    # AdaBoost documentation
â”‚   â”œâ”€â”€ gradientboost/               # Gradient Boosting docs
â”‚   â””â”€â”€ xgboost/                     # XGBoost documentation
â”‚
â”œâ”€â”€ ğŸ“ text_dataset/
â”‚   â”œâ”€â”€ basic.txt                    # Sample test data
â”‚   â””â”€â”€ how?.txt                     # Additional samples
â”‚
â”œâ”€â”€ ğŸ“ models/                       # Saved model files
â”‚   â”œâ”€â”€ FINAL_MODEL.pkl              # Production model
â”‚   â”œâ”€â”€ tfidf_vectorizer.pkl         # TF-IDF transformer
â”‚   â”œâ”€â”€ label_encoder.pkl            # Label encoder
â”‚   â”œâ”€â”€ svm_tuned.pkl                # Tuned SVM
â”‚   â”œâ”€â”€ xgboost_model_highpower.pkl  # XGBoost model
â”‚   â”œâ”€â”€ lgbm_model.pkl               # LightGBM model
â”‚   â””â”€â”€ log_reg_fast.pkl             # Logistic Regression
â”‚
â”œâ”€â”€ ğŸ“„ README.md                     # This file
â”œâ”€â”€ ğŸ“„ CHATBOT_README.md             # Chatbot documentation
â”œâ”€â”€ ğŸ“„ mainidea.txt                  # Project concept notes
â””â”€â”€ ğŸ“„ LICENSE                       # MIT License
```

---

## ğŸ¨ Visualizations

The project generates comprehensive visualizations including:

### 1ï¸âƒ£ Confusion Matrix
Displays prediction accuracy across all 7 mental health categories with actual vs predicted distributions.

### 2ï¸âƒ£ Precision-Recall Curves
Per-class PR curves with Average Precision (AP) scores:
- Normal: AP = 0.97
- Anxiety: AP = 0.87
- Bipolar: AP = 0.86
- Depression: AP = 0.83
- Suicidal: AP = 0.76

### 3ï¸âƒ£ ROC Curves
Multi-class ROC analysis showing AUC > 0.90 for most categories.

### 4ï¸âƒ£ Confidence Histogram
Distribution of model confidence for correct vs incorrect predictionsâ€”demonstrates well-calibrated probability estimates.

### 5ï¸âƒ£ Calibration Curve
Reliability diagram showing alignment between predicted probabilities and actual outcomes.

### 6ï¸âƒ£ t-SNE Visualization
2D projection of model output space revealing class clustering and separation.

### 7ï¸âƒ£ Class Weight Visualization
Bar chart displaying computed class weights (1.0 to 6.92x) for handling imbalance.

### 8ï¸âƒ£ Word Clouds
Dominant vocabulary per mental health category:
- **Depression**: "feel", "sad", "life", "hopeless"
- **Anxiety**: "panic", "worry", "heart", "racing"
- **Suicidal**: "end", "die", "want", "life"

---

## ğŸ§ª Model Validation & Overfitting Prevention

### Evidence Against Overfitting

| Validation Method | Result | Interpretation |
|:------------------|:-------|:---------------|
| **Train-Test Gap** | 1.0% | Minimal generalization gap |
| **Cross-Validation** | 3-Fold CV | Robust across data splits |
| **Holdout Validation** | 78.87% | Unbiased final evaluation |
| **Stratified Sampling** | âœ… | Class distribution preserved |

### Anti-Leakage Measures

1. **TF-IDF fit only on training data** â€” never on test
2. **Genetic Algorithm optimized on 50% of test set** â€” validated on remaining 50%
3. **No hyperparameter tuning on final holdout set**

---

## ğŸ”® Future Improvements

| Improvement | Expected Impact | Difficulty |
|:------------|:----------------|:-----------|
| ğŸ”„ **BERT Fine-tuning** | +3-5% accuracy | High |
| ğŸ“Š **Data Augmentation** | Better minority recall | Medium |
| ğŸ¯ **Threshold Optimization** | Higher Suicidal recall | Low |
| ğŸŒ **Multi-language Support** | Broader applicability | High |
| ğŸ“± **Mobile App** | Accessibility | Medium |
| ğŸ”’ **Privacy-preserving ML** | HIPAA compliance | High |
| ğŸ¤– **Chatbot Integration** | Real-time support | Medium |

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** your changes (`git commit -m 'Add AmazingFeature'`)
4. **Push** to the branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

### ğŸ“ Code Style
- Follow PEP 8 guidelines
- Add docstrings to functions
- Include unit tests for new features

---

## ğŸ‘¥ Authors

- **Likhith** - *Lead Developer* - [@Likhith623](https://github.com/Likhith623)

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## âš ï¸ Disclaimer

This tool is designed for **research and educational purposes only**. It is **NOT** a substitute for professional mental health diagnosis or treatment. If you or someone you know is struggling with mental health issues, please seek help from a qualified healthcare provider.

**Crisis Resources:**
- ğŸ‡ºğŸ‡¸ National Suicide Prevention Lifeline: 988
- ğŸ‡ºğŸ‡¸ Crisis Text Line: Text HOME to 741741
- ğŸŒ International Association for Suicide Prevention: https://www.iasp.info/resources/Crisis_Centres/

---

## ğŸ“š References

1. Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. *KDD '16*.
2. Ke, G., et al. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. *NeurIPS*.
3. Mitchell, M. (1998). An Introduction to Genetic Algorithms. *MIT Press*.
4. Pedregosa, F., et al. (2011). Scikit-learn: Machine Learning in Python. *JMLR*.
5. Rennie, J. D., et al. (2003). Tackling the Poor Assumptions of Naive Bayes Text Classifiers. *ICML*.

---

<div align="center">

### â­ Star this repo if you found it helpful!

<img src="https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png" alt="rainbow line" width="100%">

**Made with â¤ï¸ for Mental Health Awareness by Likhith**

*"Technology alone cannot solve mental health challenges, but it can be a powerful tool in early detection and support."*

</div>
